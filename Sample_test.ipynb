{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils import add_logging_arguments\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "import imageio\n",
    "import shutil\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import data\n",
    "import models\n",
    "import utils\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7312f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = lambda: None\n",
    "args.gpu = 0 # gpu_id\n",
    "args.angular = 7 # number of angular views to generate; changing this will affect quantitative evaluation\n",
    "args.model = 'unet_icip'\n",
    "args.disp_model = 'dispnetC'\n",
    "args.flow_model = 'flownetC'\n",
    "args.display = 'multilayer'\n",
    "\n",
    "args.layers = 3\n",
    "args.rank = 12\n",
    "args.restore_file = 'weights/checkpoint.pt'\n",
    "\n",
    "\n",
    "args.dataset = 'test_st' # dataloader specifically for case without GT LF images\n",
    "args.h5_file = 'stereo_data.h5'\n",
    "args.data_path = 'data'\n",
    "args.batch_size = 1\n",
    "args.save_dir = 'results'\n",
    "\n",
    "args.inph = 270 # please specify the input height and width values of your data\n",
    "args.inpw = 470\n",
    "args.seq_len = 4\n",
    "\n",
    "args.seed = 42\n",
    "args.dry_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(inputs, outputs):\n",
    "    inputs['video'] = inputs['video'].to(device)\n",
    "    inputs['lf_states'] = None\n",
    "    inputs['prev_idx'] = 0\n",
    "    inputs['curr_step'] = 0\n",
    "    outputs['pred_lf'] = []\n",
    "\n",
    "    for t in range(1, inputs['video'].size(1)):\n",
    "        # iterate over the t \\in T frames\n",
    "        if t > 0:\n",
    "            inputs['flow_loss'] = True\n",
    "            inputs['prev_idx'] = int(t - 1)\n",
    "        inputs['curr_step'] = t\n",
    "        run_instance(inputs, outputs)\n",
    "\n",
    "def run_instance(inputs, outputs):\n",
    "    # [N,2,2,3,H,W]: targets\n",
    "    # [batch, time, view, rgb, height, width]\n",
    "    targets = inputs['video'][:, inputs['prev_idx']:inputs['curr_step'] + 1, ...]\n",
    "    curr_gt_lf_frame = targets[:, -1, ...]\n",
    "    curr_stereo_frame = curr_gt_lf_frame#[:, lf_view_idx, ...]\n",
    "    instance_loss = 0.\n",
    "\n",
    "    # from the same function return the disparity map\n",
    "    decomposition = lf_model(curr_stereo_frame, inputs)\n",
    "    # decomposition is of size [N,layers,rank,3,h,w]\n",
    "    curr_lf = tensor_display(decomposition)\n",
    "    outputs['pred_lf'].append(curr_lf)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    f'cuda:{args.gpu}') if torch.cuda.is_available() else torch.device('cpu')\n",
    "utils.setup_experiment(args)\n",
    "utils.init_logging(args)\n",
    "\n",
    "# indices of the input stereo frame in the output LF\n",
    "left_view_idx = 0\n",
    "right_view_idx = 1\n",
    "\n",
    "# this is the view indices for the network to predict LF\n",
    "# lf_view_idx = [left_view_idx, right_view_idx]\n",
    "# print(f'using stereo view indices as {lf_view_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = []\n",
    "trainable_params = []\n",
    "# ============== Initialize all network models ===================\n",
    "# initialize the lf prediction network V\n",
    "lf_model = models.build_model(\n",
    "    args.model,\n",
    "    n_channels= 2 * 3,\n",
    "    args=args).to(device)\n",
    "trainable_params.extend(list(lf_model.parameters()))\n",
    "models_list.append(lf_model)\n",
    "\n",
    "# initialize the optical flow prediction network O\n",
    "flow_model = models.build_model(\n",
    "    args.flow_model,\n",
    "    n_channels=3,\n",
    "    args=args).to(device)\n",
    "trainable_params.extend(list(flow_model.parameters()))\n",
    "models_list.append(flow_model)\n",
    "\n",
    "# initialize the disparity map prediction network D\n",
    "disp_model = models.build_model(\n",
    "    args.disp_model,\n",
    "    n_channels=3,\n",
    "    args=args).to(device)\n",
    "trainable_params.extend(list(disp_model.parameters()))\n",
    "models_list.append(disp_model)\n",
    "\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "logging.info(\n",
    "    f\"Built {len(models_list)} models consisting of {sum(p.numel() for p in trainable_params):,} parameters\")\n",
    "\n",
    "# ========== Initialize the low-rank display model ==============\n",
    "if args.display == 'multilayer':\n",
    "    tensor_display = models.multilayer(\n",
    "        args.angular,\n",
    "        args.layers,\n",
    "        args.inph,\n",
    "        args.inpw,\n",
    "        args=args).to(device)\n",
    "else:\n",
    "    print('No valid display type chosen')\n",
    "    print('exiting')\n",
    "    exit(0)\n",
    "logging.info(\n",
    "    f\"Using the {args.display} display with {args.layers} layers and {args.rank} rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8baef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = utils.load_checkpoint(\n",
    "    args, models_list, optimizer, scheduler)\n",
    "global_step = state_dict['last_step']\n",
    "start_epoch = state_dict['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data.build_dataset(\n",
    "    args.dataset,\n",
    "    args.data_path,\n",
    "    args,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = args.save_dir#os.path.join('/', save_path, expt_dir)\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "    print(f'removing the directory tree {save_path}')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "print(save_path)\n",
    "\n",
    "for model in models_list:\n",
    "    model.eval()\n",
    "tensor_display.eval()\n",
    "\n",
    "test_bar = utils.ProgressBar(test_loader)\n",
    "save_every = 1\n",
    "for sample_id, inputs in enumerate(test_bar):\n",
    "    ssim_vid = []\n",
    "    lpips_vid = []\n",
    "    with torch.no_grad():\n",
    "        outputs = {}\n",
    "        run_batch(inputs, outputs)\n",
    "        # outputs[\"pred_lf\"] will be a sequence of LF frames\n",
    "        # you just have to save them\n",
    "        # and also compute psnr; ssim and lpips\n",
    "        # which you can do in the run_batch fn itself\n",
    "        # test only with a batch size of 1\n",
    "        assert inputs['video'].size(0) == 1\n",
    "        if sample_id % save_every == 0:\n",
    "            # each video sequence will be saved in a separate directory\n",
    "            # each frame of the video sequence will be saved in a separate\n",
    "            # sub-directory\n",
    "            seq_save_path = f'{save_path}/seq_{sample_id:03d}'\n",
    "            os.makedirs(f'{save_path}/seq_{sample_id:03d}', exist_ok=True)\n",
    "\n",
    "            # then save the predicted light field\n",
    "            for t in range(len(outputs['pred_lf'])):\n",
    "                pred_lf_np = outputs['pred_lf'][t].data.cpu(\n",
    "                ).numpy().squeeze()\n",
    "                pred_lf_np = np.transpose(pred_lf_np, [0, 2, 3, 1])\n",
    "                pred_lf_np = (pred_lf_np - np.amin(pred_lf_np))/(np.amax(pred_lf_np) - np.amin(pred_lf_np))\n",
    "                save_lf_path = os.path.join(\n",
    "                    seq_save_path, f'pred_lf_{sample_id:02d}_{t:02d}.avi')\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                out = cv2.VideoWriter(\n",
    "                    save_lf_path, fourcc, 7, (args.inpw, args.inph))\n",
    "                for k in range(len(pred_lf_np)):\n",
    "                    out.write(np.uint8(pred_lf_np[k, ..., ::-1] * 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e0277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
